{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashaaankk/GradientAscent/blob/main/GradientAscent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gpxpy\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOCAL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 41
        },
        "id": "Mngq1luAl8g2",
        "outputId": "39508712-f216-4b99-b364-a33f4e9681f9"
      },
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    !pip install --quiet kaggle kagglehub[pandas-datasets]\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()   # click to select your kaggle.json\n",
        "    if 'kaggle.json' not in uploaded:\n",
        "        raise FileNotFoundError(\"You must upload the kaggle.json you downloaded from Kaggle.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X7oK6Nazs6qJ"
      },
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    import os, shutil\n",
        "    # make sure ~/.kaggle exists\n",
        "    kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "    os.makedirs(kaggle_dir, exist_ok=True)\n",
        "\n",
        "    # move and secure\n",
        "    shutil.move(\"kaggle.json\", os.path.join(kaggle_dir, \"kaggle.json\"))\n",
        "    os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\n",
        "\n",
        "    # sometimes needed:\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = kaggle_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OqULn93ftgIl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A subdirectory or file data already exists.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\n",
            "  0 57.4M    0 79664    0     0   108k      0  0:09:02 --:--:--  0:09:02  108k\n",
            " 22 57.4M   22 12.9M    0     0  7748k      0  0:00:07  0:00:01  0:00:06 12.9M\n",
            " 60 57.4M   60 34.5M    0     0  12.7M      0  0:00:04  0:00:02  0:00:02 17.3M\n",
            " 96 57.4M   96 55.6M    0     0  14.9M      0  0:00:03  0:00:03 --:--:-- 18.5M\n",
            "100 57.4M  100 57.4M    0     0  15.1M      0  0:00:03  0:00:03 --:--:-- 18.7M\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "if not LOCAL:\n",
        "    import kagglehub\n",
        "    path = kagglehub.dataset_download(\"roccoli/gpx-hike-tracks\")\n",
        "else:\n",
        "    !mkdir data\n",
        "    !curl -L -o ./data/gpx-hike-tracks.zip https://www.kaggle.com/api/v1/datasets/download/roccoli/gpx-hike-tracks\n",
        "    !unzip -o data/gpx-hike-tracks.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h2u_1MMRuUSn"
      },
      "outputs": [],
      "source": [
        "# 1. Install gpxpy (to parse .gpx files)\n",
        "import gpxpy\n",
        "import pandas as pd\n",
        "if not LOCAL:\n",
        "    !pip install --quiet gpxpy\n",
        "\n",
        "    # 2. Import libs\n",
        "    import os, glob\n",
        "\n",
        "\n",
        "    csv_files = glob.glob(os.path.join(path, \"**\", \"*.csv\"), recursive=True)\n",
        "    csv_path = csv_files[0]\n",
        "    print(\"Loading:\", csv_path)\n",
        "\n",
        "else:\n",
        "    csv_path = \"data/gpx-tracks-from-hikr.org.csv\"\n",
        "\n",
        "# Read and inspect\n",
        "df = pd.read_csv(csv_path)\n",
        "# print(\"Shape:\", df.shape)\n",
        "# print(\"Columns:\", df.columns.tolist())\n",
        "# print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting distance between start and end points of GPX tracks\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371000  # radius in meters\n",
        "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
        "    dphi = math.radians(lat2 - lat1)\n",
        "    dlambda = math.radians(lon2 - lon1)\n",
        "    a = math.sin(dphi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2)**2\n",
        "    return R * 2 * math.asin(math.sqrt(a))\n",
        "\n",
        "\n",
        "def start_end_distance_from_gpx_string(gpx_string):\n",
        "    \"\"\"\n",
        "    Parse a GPX XML string, extract the first and last trkpt coordinates,\n",
        "    and return the Haversine distance between them in kilometers.\n",
        "    Returns None if parsing fails or fewer than 2 track points.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        root = ET.fromstring(gpx_string)\n",
        "        trkpts = []\n",
        "        for elem in root.iter():\n",
        "            if elem.tag.endswith('trkpt'):\n",
        "                lat = elem.attrib.get('lat')\n",
        "                lon = elem.attrib.get('lon')\n",
        "                if lat is not None and lon is not None:\n",
        "                    trkpts.append((float(lat), float(lon)))\n",
        "        if len(trkpts) < 2:\n",
        "            return None\n",
        "        lat1, lon1 = trkpts[0]\n",
        "        lat2, lon2 = trkpts[-1]\n",
        "        dist_mm = haversine(lat1, lon1, lat2, lon2)\n",
        "        return dist_mm \n",
        "    except ET.ParseError:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mn1XCOEBw6hP"
      },
      "outputs": [],
      "source": [
        "#Pre-processing 1\n",
        "\n",
        "df = df.dropna()\n",
        "# Convert time columns to datetime\n",
        "df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], format=\"%Y-%m-%d %H:%M:%S\" , errors='coerce')\n",
        "df[\"end_time\"] = pd.to_datetime(df[\"end_time\"], format=\"%Y-%m-%d %H:%M:%S\" ,errors='coerce')\n",
        "\n",
        "# Compute total duration in seconds\n",
        "df[\"duration\"] =  df[\"moving_time\"]\n",
        "\n",
        "# Compute break time: duration - moving_time\n",
        "df[\"break_time\"] = (df[\"end_time\"] - df[\"start_time\"]).dt.total_seconds() - df[\"moving_time\"]\n",
        "\n",
        "\n",
        "df['start_end_distance'] = df['gpx'].apply(start_end_distance_from_gpx_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'gpx'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m df_example = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mgpx\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Apply slope stats function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m df_stats = \u001b[43mdf_example\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(slope_stats_from_gpx_string).apply(pd.Series)\n\u001b[32m     92\u001b[39m df_result = pd.concat([df_example, df_stats], axis=\u001b[32m1\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExample slope stats:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priyansh\\anaconda3\\envs\\py_ml_313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\priyansh\\anaconda3\\envs\\py_ml_313\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'gpx'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import math\n",
        "\n",
        "# Haversine distance for horizontal distance calculation\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371000  # radius in meters\n",
        "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
        "    dphi = math.radians(lat2 - lat1)\n",
        "    dlambda = math.radians(lon2 - lon1)\n",
        "    a = math.sin(dphi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2)**2\n",
        "    return R * 2 * math.asin(math.sqrt(a))\n",
        "\n",
        "def slope_stats_from_gpx_string(gpx_string):\n",
        "    \"\"\"\n",
        "    Parse GPX XML string, extract track points, compute slopes between consecutive points.\n",
        "    Returns a dict with statistics for ascent and descent slopes (in percent).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        root = ET.fromstring(gpx_string)\n",
        "        pts = []\n",
        "        for elem in root.iter():\n",
        "            if elem.tag.endswith('trkpt'):\n",
        "                lat = elem.attrib.get('lat')\n",
        "                lon = elem.attrib.get('lon')\n",
        "                ele_elem = elem.find('{*}ele')\n",
        "                if lat is not None and lon is not None and ele_elem is not None:\n",
        "                    try:\n",
        "                        ele = float(ele_elem.text)\n",
        "                    except (TypeError, ValueError):\n",
        "                        ele = None\n",
        "                    if ele is not None:\n",
        "                        pts.append((float(lat), float(lon), ele))\n",
        "        if len(pts) < 2:\n",
        "            return None  # insufficient points\n",
        "        ascent_slopes = []\n",
        "        descent_slopes = []\n",
        "        for i in range(1, len(pts)):\n",
        "            lat1, lon1, ele1 = pts[i-1]\n",
        "            lat2, lon2, ele2 = pts[i]\n",
        "            horiz = haversine(lat1, lon1, lat2, lon2)\n",
        "            if horiz <= 0:\n",
        "                continue\n",
        "            ele_diff = ele2 - ele1\n",
        "            slope_pct = (ele_diff / horiz) * 100  # percent grade\n",
        "            if ele_diff > 0:\n",
        "                ascent_slopes.append(slope_pct)\n",
        "            elif ele_diff < 0:\n",
        "                descent_slopes.append(abs(slope_pct))\n",
        "        stats = {\n",
        "            'mean_ascent_slope_pct': None,\n",
        "            'max_ascent_slope_pct': None,\n",
        "            'median_ascent_slope_pct': None,\n",
        "            'count_ascent_segments': 0,\n",
        "            'mean_descent_slope_pct': None,\n",
        "            'max_descent_slope_pct': None,\n",
        "            'median_descent_slope_pct': None,\n",
        "            'count_descent_segments': 0\n",
        "        }\n",
        "        if ascent_slopes:\n",
        "            stats.update({\n",
        "                'mean_ascent_slope_pct': sum(ascent_slopes) / len(ascent_slopes),\n",
        "                'max_ascent_slope_pct': max(ascent_slopes),\n",
        "                'median_ascent_slope_pct': pd.Series(ascent_slopes).median(),\n",
        "                'count_ascent_segments': len(ascent_slopes)\n",
        "            })\n",
        "        if descent_slopes:\n",
        "            stats.update({\n",
        "                'mean_descent_slope_pct': sum(descent_slopes) / len(descent_slopes),\n",
        "                'max_descent_slope_pct': max(descent_slopes),\n",
        "                'median_descent_slope_pct': pd.Series(descent_slopes).median(),\n",
        "                'count_descent_segments': len(descent_slopes)\n",
        "            })\n",
        "        return stats\n",
        "    except ET.ParseError:\n",
        "        return None\n",
        "\n",
        "# Example: sample GPX snippet\n",
        "gpx_snippet = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<gpx xmlns=\"http://www.topografix.com/GPX/1/1\">\n",
        "  <trk><trkseg>\n",
        "    <trkpt lat=\"47.231143\" lon=\"13.227007\"><ele>1322.96</ele></trkpt>\n",
        "    <trkpt lat=\"47.230543\" lon=\"13.227488\"><ele>1330.89</ele></trkpt>\n",
        "    <trkpt lat=\"47.23103\" lon=\"13.226896\"><ele>1326.17</ele></trkpt>\n",
        "  </trkseg></trk>\n",
        "</gpx>\"\"\"\n",
        "\n",
        "# Create example DataFrame\n",
        "df_example = pd.DataFrame({'gpx': [gpx_snippet]})\n",
        "# Apply slope stats function\n",
        "df_stats = df_example['gpx'].apply(slope_stats_from_gpx_string).apply(pd.Series)\n",
        "df_result = pd.concat([df_example, df_stats], axis=1)\n",
        "print(\"Example slope stats:\")\n",
        "print(df_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Pre-processing 0 (get rid of values with equal start and end time)\n",
        "\n",
        "try:\n",
        "    # Convert to datetime if not already\n",
        "    df['start_time'] = pd.to_datetime(df['start_time'], errors='coerce')\n",
        "    df['end_time'] = pd.to_datetime(df['end_time'], errors='coerce')\n",
        "    \n",
        "    # Drop rows where start_time equals end_time\n",
        "    initial_count = len(df)\n",
        "    df = df[df['start_time'] != df['end_time']].copy()\n",
        "    dropped_count = initial_count - len(df)\n",
        "    \n",
        "    # Display result summary and first few rows\n",
        "    print(f\"Dropped {dropped_count} rows where start_time == end_time.\")\n",
        "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"Filtered DataFrame\", dataframe=df.head())\n",
        "except NameError:\n",
        "    print(\"DataFrame 'df' is not defined. Please ensure your DataFrame is named 'df' and has 'start_time' and 'end_time' columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Select relevant features\n",
        "selected = df[[\"duration\",\"length_3d\", \"min_elevation\", \"max_elevation\", \"break_time\", \"uphill\", \"downhill\"]]\n",
        "\n",
        "X = selected\n",
        "y = df['difficulty'].str[1].astype(int)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train logistic regression\n",
        "mask = X.notnull().all(axis=1) & y.notnull()\n",
        "X_clean = X_scaled[mask]\n",
        "y_clean = y[mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwQ84hlNyIut"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THfTKXxXxE4D"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_clean = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_clean)\n",
        "# explained = pca.explained_variance_ratio_"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "py_ml_313",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
